Numerical Optimizers from Scratch ðŸš€


This repository provides pure Python implementations of fundamental optimization algorithms used in machine learning and numerical computing. Each optimizer is built from scratch with clear mathematical foundations.


ðŸ“Œ Available Optimizers
1. First-Order Gradient-Based Methods:
   Batch Gradient Descent
   Stochastic GD (SGD)
   Mini-Batch GD
   Momentum
   NAG (Nesterov)
3. Adaptive Learning Rate Methods:
   Adagrad
   RMSprop
   Adam
4. Second-Order optimization Methods:
   Newton's Method
   BFGS
 
